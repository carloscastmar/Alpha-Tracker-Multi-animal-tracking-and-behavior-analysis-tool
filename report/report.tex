% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  11pt,
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Master Research Project Report},
  pdfauthor={Carlos Castillo},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=3cm]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

\title{Master Research Project Report}
\author{Carlos Castillo}
\date{}

\begin{document}
\maketitle

\pagebreak
\tableofcontents
\pagebreak

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\hypertarget{project-overview}{%
\subsection{Project Overview}\label{project-overview}}

AlphaTracker is a groundbreaking multi-animal tracking and behavioral
analysis tool that revolutionizes system neuroscience research. Designed
to empower researchers, it incorporates advanced features such as
multi-animal tracking, pose estimation, and unsupervised behavioral
clustering. AlphaTracker sets new standards in accuracy for multi-animal
tracking, providing a strong foundation for rigorous biological studies.

The application's exceptional capabilities stem from cutting-edge
computer vision and machine learning techniques. By seamlessly
integrating multi-animal tracking and pose estimation, AlphaTracker
enables researchers to gain deeper insights into the complex
interactions and behaviors of multiple subjects in real-time video
streams.

One of the key strengths of AlphaTracker lies in its adaptability to
diverse research settings. The application is designed to operate with
minimal hardware requirements, making use of regular webcams, and boasts
an efficient training procedure. This ease of adoption facilitates
widespread usage across various neuroscience laboratories, ensuring
researchers can readily harness its potential for their experiments.

The AlphaTracker project is the result of the collective efforts of a
highly skilled and collaborative team, comprising computer vision
engineers, machine learning experts, and software developers. Each
member's expertise plays a crucial role in developing specific
components of the application, culminating in an integrated and
high-performing solution.

Within this report, the focus centers on my individual contributions to
the AlphaTracker project. It presents in-depth insights into the
methodologies I employed, the challenges I encountered, and the results
achieved during my involvement. By sharing my accomplishments, I aim to
highlight the significance of my work within the team and demonstrate
how it contributes to the success of AlphaTracker as a groundbreaking
tool for advancing system neuroscience research.

\hypertarget{role-and-responsibilities}{%
\subsection{Role and Responsibilities}\label{role-and-responsibilities}}

As a member of the support group for AlphaTracker, I was part of a team
of master's degree students working closely with the main developers,
who were primarily PhD researchers. Our role in the project was vital in
assisting the core team with various essential tasks, contributing to
the overall success of the application's development and refinement. Our
responsibilities encompassed a diverse range of activities, each
tailored to complement the expertise of the main developers and ensure
the efficient progression of the project.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Data Preparation: One of our key responsibilities was to handle data
  preparation, including data collection, cleaning, and organization. We
  ensured that the datasets used for model training and evaluation were
  curated meticulously to provide accurate and reliable inputs for the
  algorithms.
\item
  Model Training, Correction, and Visualization: Our team actively
  engaged in model training, fine-tuning, and corrections. We worked
  collaboratively to optimize the performance of the tracking and pose
  estimation models, employing cutting-edge computer vision and machine
  learning techniques. Additionally, we developed visualizations to
  analyze model outputs and identify potential areas for improvement.
\item
  Metric Development and Testing: Another crucial aspect of our role was
  the development and testing of various evaluation metrics. We designed
  and implemented metrics to assess the tracking accuracy, pose
  estimation precision, and clustering performance, thereby contributing
  to the quantitative evaluation of the application's capabilities.
\item
  Evaluation of Pose Estimation: We dedicated effort to rigorously
  evaluating the pose estimation functionality within AlphaTracker. By
  conducting thorough assessments and comparisons with ground truth
  data, we helped ensure the accuracy and reliability of the pose
  estimation algorithms.
\item
  Clustering Algorithm Testing: Our team also played a significant role
  in testing different clustering algorithms, both for visual and audio
  data. This involved conducting experiments and analyzing results to
  determine the most effective approach for behavioral clustering.
\item
  Documentation and Reporting: Throughout the project, we diligently
  documented our work, methodologies, and results. We prepared detailed
  reports and contributed to the project's documentation, providing
  clear and comprehensive insights into our contributions and findings.
\item
  Continuous Collaboration and Support: As a support group, we
  maintained open and effective communication with the main developers.
  We actively participated in team meetings, discussions, and
  brainstorming sessions, offering valuable inputs to enhance the
  application's functionality and addressing challenges collaboratively.
\end{enumerate}

Our contributions as a support group significantly enriched the
development process of AlphaTracker, helping to bridge the gap between
research and application. By taking on diverse responsibilities and
leveraging our skills, we played a crucial role in the success of the
project, contributing to the advancement of system neuroscience research
and the empowering tools for the scientific community.

\hypertarget{scope-of-work}{%
\subsection{Scope of Work}\label{scope-of-work}}

As a member of a project support group, my role is to assist developers
with tasks as needed. Not having belonged to the project since its
inception, the main tasks in which I have given support have not been
related to software development of the application itself, but have been
more related to more technically simple and experimental tasks. In any
case, the understanding of the operation of the application and its
software structure has been necessary to perform these tasks. Finally,
the objective of this project also lies in learning the techniques used
in the project and the appropriate methodology for the development of a
technological project of this category.

\hypertarget{alphatracker-description}{%
\section{AlphaTracker Description}\label{alphatracker-description}}

The AlphaTracker pipeline consists of three main stages: tracking
(AlphaTracker), behavioral clustering, and result analysis with a
customized user interface (UI). AlphaTracker allows for multi-animal
tracking on videos recorded via webcam, making it convenient and
cost-effective for laboratory settings. The behavioral clustering stage
enables unbiased identification of behavioral motifs, with results
further reviewed and error-corrected using a customized UI.

The tracking component (AlphaTracker) is adapted from AlphaPose15, a
human pose estimation algorithm. It comprises three steps: animal
detection using YOLOv316, keypoint estimation using
Squeeze-and-Excitation Networks (SENet)17, and identity (ID) tracking
across frames. The pipeline proposes a novel target association method
to consistently track IDs of nearly identical animals by defining
descriptors for animal positions and orientations and calculating
similarities between descriptors for tracking.

To address inaccuracies due to tracking errors or occlusion, the
pipeline utilizes Kalman filtering21 to predict keypoint locations in
consecutive frames. This filtering technique models motion with velocity
and acceleration, providing enhanced tracking accuracy when users amend
results in the UI.

The pipeline's innovative approach to multi-animal tracking, behavioral
clustering, and result analysis using a user-friendly interface
demonstrates its potential to advance system neuroscience research and
empower researchers in their studies of animal behavior.
{[}@alphatracker{]}

\hypertarget{tracking}{%
\subsection{Tracking}\label{tracking}}

AlphaTracker demonstrates reliable performance in tracking multiple
unmarked animals. It excels in complex environments like home cages with
bedding and metal operant chambers, achieving high accuracy.

The tool handles challenges posed by occlusion due to head implants,
maintaining robust performance. It also exhibits good tolerance for
low-resolution videos from webcams (576p), enabling continuous
monitoring over extended periods.

In social neuroscience research, AlphaTracker efficiently tracks four
identical-looking mice during home cage interaction.

The model can be quickly and easily trained, involving two steps:
training the animal detector and training the pose estimator. Users can
modify training hyperparameters to optimize performance without
overfitting risks.

AlphaTracker offers a rapid and accessible solution for neuroscience
labs, as testing showed quick training times (0.2 seconds for a batch of
10 images and 30 minutes for 6000 training images) on compatible
hardware. {[}@alphatracker{]}

\hypertarget{clustering}{%
\subsection{Clustering}\label{clustering}}

AlphaTracker's behavioral clustering allows unsupervised clustering of
individual and social behaviors, minimizing human bias. Features
extracted from keypoints and binary masks are input to the hierarchical
clustering algorithm. It successfully captures various behaviors and
facilitates associative analysis between behavior motifs and
experimental factors. The algorithm's performance is validated using the
Adjusted Rand Index (ARI), showing improved results with larger
datasets. AlphaTracker can be operated on Google Colab without GPUs,
offering accessibility and comparable performance to local GPU-enabled
setups. Detailed usage guidance can be found in the project's Github
repository. {[}@alphatracker{]}

\hypertarget{user-interface}{%
\subsection{User Interface}\label{user-interface}}

Customized user interfaces (UIs) for inspecting and revising tracking
and clustering results were designed to address potential errors in the
deep learning framework. The web-based UIs run on systems with a
pre-installed Python environment.

The tracking UI enables users to scroll through the video, inspect
keypoints, and verify mouse IDs over time. The event timeline
facilitates error identification, displaying the confidence score to
indicate periods of low confidence. Users can easily correct errors by
dragging keypoints or reassigning identities. The ``curate'' function
automatically corrects subsequent frames based on user adjustments.

The clustering UI features an interactive dendrogram to display
clustering results. Users can choose to view broader cluster assignments
or explore details. The scatterplot in the timeline visualizes clip
distributions of specific clusters. Users can rename clusters, modify
assignments, merge clusters, and save curated results in JSON format for
further analysis. {[}@alphatracker{]}

\hypertarget{tasks-description}{%
\section{Tasks Description}\label{tasks-description}}

\hypertarget{task-x-task-name}{%
\subsection{Task X: {[}Task Name{]}}\label{task-x-task-name}}

\hypertarget{description-of-the-task}{%
\subsubsection{Description of the task}\label{description-of-the-task}}

\hypertarget{approach-and-methodology-used}{%
\subsubsection{Approach and Methodology
Used}\label{approach-and-methodology-used}}

\hypertarget{challenges-encountered}{%
\subsubsection{Challenges Encountered}\label{challenges-encountered}}

\hypertarget{solutions-implemented}{%
\subsubsection{Solutions Implemented}\label{solutions-implemented}}

\hypertarget{results-and-outcomes}{%
\subsubsection{Results and Outcomes}\label{results-and-outcomes}}

\hypertarget{summary-of-contributions}{%
\section{Summary of Contributions}\label{summary-of-contributions}}

\hypertarget{overview-of-my-overall-impact-on-the-project}{%
\subsection{Overview of my Overall Impact on the
Project}\label{overview-of-my-overall-impact-on-the-project}}

\hypertarget{key-accomplishments}{%
\subsection{Key Accomplishments}\label{key-accomplishments}}

\hypertarget{challenges-overcome}{%
\subsection{Challenges Overcome}\label{challenges-overcome}}

\hypertarget{lessons-learned}{%
\section{Lessons Learned}\label{lessons-learned}}

\hypertarget{skills-developed-or-enhanced}{%
\subsection{Skills Developed or
Enhanced}\label{skills-developed-or-enhanced}}

\hypertarget{knowledge-gained}{%
\subsection{Knowledge Gained}\label{knowledge-gained}}

\hypertarget{reflections-on-the-project-experience}{%
\subsection{Reflections on the Project
Experience}\label{reflections-on-the-project-experience}}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

\hypertarget{recap-of-my-contributions}{%
\subsection{Recap of my Contributions}\label{recap-of-my-contributions}}

\hypertarget{acknowledgment-of-team-members}{%
\subsection{Acknowledgment of Team
Members}\label{acknowledgment-of-team-members}}

\hypertarget{future-directions-or-recommendations}{%
\subsection{Future Directions or
Recommendations}\label{future-directions-or-recommendations}}

\hypertarget{references}{%
\section{References}\label{references}}

@article\{alphatracker, author=\{Zexin Chen\}, title=\{AlphaTracker: A
Multi-Animal Tracking and Behavioral Analysis Tool\}, year=\{2020\}, \}

\hypertarget{appendices}{%
\section{Appendices}\label{appendices}}

\hypertarget{additional-supporting-material-if-necessary}{%
\subsection{Additional Supporting Material (if
necessary)}\label{additional-supporting-material-if-necessary}}

\hypertarget{code-snippets}{%
\subsection{Code Snippets}\label{code-snippets}}

\end{document}
